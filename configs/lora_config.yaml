# LoRA Training Configuration
# Based on x-flux LoRA training setup

# Model Configuration
model_name: "flux-dev"

# Data Configuration
data_config:
  train_batch_size: 1
  num_workers: 4
  img_size: 512
  img_dir: "data/images/"
  random_ratio: true  # Support multi crop preprocessing

# Training Parameters
train_batch_size: 1
max_train_steps: 100000
learning_rate: 1e-5
lr_scheduler: "constant"
lr_warmup_steps: 10

# Optimizer Configuration
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1e-8
max_grad_norm: 1.0

# Training Strategy
gradient_accumulation_steps: 2
mixed_precision: "bf16"

# Checkpointing and Saving
output_dir: "lora/"
checkpointing_steps: 2500
checkpoints_total_limit: 10
resume_from_checkpoint: "latest"

# Logging Configuration
logging_dir: "logs"
report_to: "wandb"
tracker_project_name: "lora_training"

# LoRA Specific Configuration
lora_config:
  rank: 16
  single_blocks: "1,2,3,4"  # Which blocks to apply LoRA to
  double_blocks: null
  disable_sampling: false
  
# Sampling Configuration
sampling:
  sample_every: 250  # Sample every this many steps
  sample_width: 1024
  sample_height: 1024
  sample_steps: 20
  sample_prompts:
    - "woman with red hair, playing chess at the park, bomb going off in the background"
    - "a woman holding a coffee cup, in a beanie, sitting at a cafe"

# Paths Configuration
paths:
  output_dir: "lora/"
  logging_dir: "logs"
  checkpoint_dir: "checkpoints"
  sample_dir: "samples"
  dataset_dir: "data/images/"

# Hardware Configuration
num_gpus: 1
strategy: "auto"

# Advanced Training Options
advanced:
  use_xformers: true
  enable_attention_slicing: false
  enable_vae_slicing: false
  enable_vae_tiling: false
  gradient_checkpointing: true
  use_ema: false
  ema_decay: 0.9999 